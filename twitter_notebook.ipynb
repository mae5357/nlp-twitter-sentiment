{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/madis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/madis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/madis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from pprint import pprint  # pretty-printer\n",
    "import gensim\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def data_to_lists(folder_path):\n",
    "    assert folder_path[-1] == '/'\n",
    "    train_list_of_texts = []\n",
    "    train_list_of_targets = []\n",
    "\n",
    "    with open(folder_path+'train.csv', 'r') as file:\n",
    "        my_reader = csv.reader(file, delimiter=',')\n",
    "\n",
    "        # skip header\n",
    "        next(my_reader)\n",
    "\n",
    "        for row in my_reader:\n",
    "            train_list_of_texts.append(row[3])\n",
    "            train_list_of_targets.append(row[4])\n",
    "\n",
    "    test_list_of_texts = []\n",
    "\n",
    "    with open(folder_path+'test.csv', 'r') as file:\n",
    "        my_reader = csv.reader(file, delimiter=',')\n",
    "\n",
    "        # skip header\n",
    "        next(my_reader)\n",
    "\n",
    "        for row in my_reader:\n",
    "            test_list_of_texts.append(row[3])\n",
    "\n",
    "    return train_list_of_texts, train_list_of_targets, test_list_of_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train_list_of_texts, train_list_of_targets, test_list_of_texts = data_to_lists('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "During the preprocessing, we will remove the stop words, the punctuation and the numbers. We will also lemmatize the words. the output will be a list of words. Once we have the list of lists, we will split into train and validation sets.\n",
    "\n",
    "ex:\n",
    "```\n",
    "clean_text = [['deed', 'reason', 'earthquake', 'allah', 'forgive'],\n",
    " ['forest', 'ronge', 'canada'],\n",
    " ['resident','asked','shelter','place','notified','officer','evacuation','shelter','place','order','expected'] ...]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "def get_lemma2(word):\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    This function takes a word and returns its rootword\n",
    "    \"\"\"\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "# remove extra characters\n",
    "import re\n",
    "def clean_chr(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def nltk_preprocessing(text):\n",
    "    \"\"\"\n",
    "    This function takes a text and returns a list of tokens\n",
    "    - lowercase\n",
    "    - remove short words\n",
    "    - remove stopwords\n",
    "    - remove extra characters\n",
    "    - gets root word (lemma)\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if len(token) > 4]      # remove short words\n",
    "    tokens = [token for token in tokens if token not in en_stop] # remove stopwords\n",
    "    tokens = [clean_chr(token) for token in tokens]              # remove extra characters\n",
    "    tokens = [get_lemma2(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "clean_train_text = [nltk_preprocessing(text) for text in train_list_of_texts]\n",
    "clean_test_text = [nltk_preprocessing(text) for text in test_list_of_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_val = train_test_split(clean_train_text, train_list_of_targets, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the word2vec model\n",
    "\n",
    "We will train a Word2Vec model that will learn the word embeddings for our training data.\n",
    "\n",
    "Word2Vec is a shallow, two-layer neural network that is trained to reconstruct linguistic contexts of words. The model can either be trained on the skip-gram architecture or on the continuous bag of words (CBOW) architecture. The skip-gram architecture is used to predict the context given a word while the CBOW architecture is used to predict the word given its context.\n",
    "\n",
    "For each tweet, we will create a vector that will be the average of the word embeddings of the words in the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "n_space = 500\n",
    "# train word2vec model\n",
    "model_w2v = word2vec.Word2Vec(clean_train_text, size=n_space, window=5, min_count=1, workers=4)\n",
    "model_w2v.build_vocab(clean_train_text, update=True)\n",
    "\n",
    "# train using w2v\n",
    "model_w2v.train(clean_train_text, total_examples=len(X_train), epochs=1)\n",
    "\n",
    "# save model\n",
    "model_w2v.save('models/model_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_216453/2207460555.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vec += model_w2v[word].reshape((1, size))\n",
      "/tmp/ipykernel_216453/2207460555.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vec += model_w2v[word].reshape((1, size))\n"
     ]
    }
   ],
   "source": [
    "# scale the data for training\n",
    "from sklearn.preprocessing import scale\n",
    "train_vecs = np.concatenate([buildWordVector(z, n_space) for z in X_train])\n",
    "train_vecs = scale(train_vecs)\n",
    "\n",
    "val_vecs = np.concatenate([buildWordVector(z, n_space) for z in X_test])\n",
    "val_vecs = scale(val_vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `train_vecs` array is a 2D array where each row is a vector representation of a tweet (the average of the word vectors in the tweet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using ada boost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# make a function to train and test the model\n",
    "def train_and_test_model(model, train_vecs, train_labels, val_vecs, val_labels):\n",
    "    # train model\n",
    "    model.fit(train_vecs, train_labels)\n",
    "\n",
    "    # get training accuracy\n",
    "    train_preds = model.predict(train_vecs)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "\n",
    "    print('Training accuracy: {}'.format(train_acc))\n",
    "    # get validation accuracy\n",
    "    y_pred = model.predict(val_vecs)\n",
    "    val_acc = accuracy_score(val_labels, y_pred)\n",
    "\n",
    "    print('Val accuracy: {}'.format(val_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6100581722649653\n",
      "Val accuracy: 0.6077057793345009\n"
     ]
    }
   ],
   "source": [
    "# train very basic classification\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model_nb = GaussianNB()\n",
    "model_nb = train_and_test_model(model_nb, train_vecs, y_train, val_vecs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8070932632764121\n",
      "Val accuracy: 0.6808231173380035\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200\n",
    ")\n",
    "\n",
    "clf = train_and_test_model(clf, train_vecs, y_train, val_vecs, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ada boost seems to have a pretty good training score, however the validation score is not that good. This suggests the model is overfitting. We will try to improve the model by tuning the hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "234f4b08dea796048dbaea19d53a00081c31de7324785530dd8e3f9d12a3f19d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
