{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text = 'Hi my name is Madi and i like to code!!!'\n",
    "\n",
    "data = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "def get_lemma2(word):\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    This function takes a word and returns its rootword\n",
    "    \"\"\"\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "def nltk_preprocessing(text):\n",
    "    \"\"\"\n",
    "    This function takes a text and returns a list of tokens\n",
    "    - lowercase\n",
    "    - remove short words\n",
    "    - remove stopwords\n",
    "    - gets root word (lemma)\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # tokens = [token for token in tokens if len(token) > 4]      # remove short words\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma2(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "def make_bigrams(texts):\n",
    "    bigram = Phrases(texts, min_count=20)\n",
    "    for idx in range(len(texts)):\n",
    "        for token in bigram[texts[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                texts[idx].append(token)\n",
    "    return texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = corpora.Dictionary([nltk_preprocessing(line) for line in data['text']])\n",
    "corpus = [mydict.doc2bow(nltk_preprocessing(line)) for line in data['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scipy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus tfidf to scipy sparse matrix\n",
    "from gensim.matutils import corpus2csc\n",
    "import scipy.sparse\n",
    "corpus_sparse = scipy.sparse.csc_matrix(corpus2csc(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `corpus sparse matrix` is a scipy sparse matrix. It is a matrix of shape `(n_documents, n_words)` where each cell is the number of times a word appears in a document. The matrix is sparse because most cells are zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coo (21875, 7613)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madis/sandbox/cohere-takehome/venv/lib/python3.8/site-packages/scipy/sparse/compressed.py:80: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arg1 = np.asarray(arg1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('O'),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/sandbox/cohere-takehome/venv/lib/python3.8/site-packages/scipy/sparse/base.py:322\u001b[0m, in \u001b[0;36mspmatrix.asformat\u001b[0;34m(self, format, copy)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_method(copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    323\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/sandbox/cohere-takehome/venv/lib/python3.8/site-packages/scipy/sparse/coo.py:404\u001b[0m, in \u001b[0;36mcoo_matrix.tocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    403\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty_like(col, dtype\u001b[39m=\u001b[39midx_dtype)\n\u001b[0;32m--> 404\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty_like(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, dtype\u001b[39m=\u001b[39mupcast(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n\u001b[1;32m    406\u001b[0m coo_tocsr(M, N, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnnz, row, col, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata,\n\u001b[1;32m    407\u001b[0m           indptr, indices, data)\n",
      "File \u001b[0;32m~/sandbox/cohere-takehome/venv/lib/python3.8/site-packages/scipy/sparse/sputils.py:51\u001b[0m, in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m---> 51\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mno supported conversion for types: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (args,))\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'),)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# convert sparse matrix to csr matrix\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix\n\u001b[0;32m----> 8\u001b[0m corpus_csr \u001b[38;5;241m=\u001b[39m csr_matrix(corpus)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,corpus_csr\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/sandbox/cohere-takehome/venv/lib/python3.8/site-packages/scipy/sparse/compressed.py:85\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munrecognized \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_matrix constructor usage\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat))\n\u001b[1;32m     84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcoo\u001b[39;00m \u001b[39mimport\u001b[39;00m coo_matrix\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_self(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m(coo_matrix(arg1, dtype\u001b[39m=\u001b[39;49mdtype)))\n\u001b[1;32m     87\u001b[0m \u001b[39m# Read matrix dimensions given, if any\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/sandbox/cohere-takehome/venv/lib/python3.8/site-packages/scipy/sparse/compressed.py:34\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     32\u001b[0m         arg1 \u001b[39m=\u001b[39m arg1\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     33\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m         arg1 \u001b[39m=\u001b[39m arg1\u001b[39m.\u001b[39;49masformat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat)\n\u001b[1;32m     35\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_self(arg1)\n\u001b[1;32m     37\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg1, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/sandbox/cohere-takehome/venv/lib/python3.8/site-packages/scipy/sparse/base.py:324\u001b[0m, in \u001b[0;36mspmatrix.asformat\u001b[0;34m(self, format, copy)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_method(copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    323\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_method()\n",
      "File \u001b[0;32m~/sandbox/cohere-takehome/venv/lib/python3.8/site-packages/scipy/sparse/coo.py:404\u001b[0m, in \u001b[0;36mcoo_matrix.tocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    402\u001b[0m indptr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(M \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39midx_dtype)\n\u001b[1;32m    403\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty_like(col, dtype\u001b[39m=\u001b[39midx_dtype)\n\u001b[0;32m--> 404\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty_like(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, dtype\u001b[39m=\u001b[39mupcast(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n\u001b[1;32m    406\u001b[0m coo_tocsr(M, N, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnnz, row, col, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata,\n\u001b[1;32m    407\u001b[0m           indptr, indices, data)\n\u001b[1;32m    409\u001b[0m x \u001b[39m=\u001b[39m csr_matrix((data, indices, indptr), shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/sandbox/cohere-takehome/venv/lib/python3.8/site-packages/scipy/sparse/sputils.py:51\u001b[0m, in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         _upcast_memo[\u001b[39mhash\u001b[39m(args)] \u001b[39m=\u001b[39m t\n\u001b[1;32m     49\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m---> 51\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mno supported conversion for types: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (args,))\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'),)"
     ]
    }
   ],
   "source": [
    "# convert sparse matrix to coo matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "corpus_coo = coo_matrix(corpus_sparse)\n",
    "print(\"coo\",corpus_coo.shape)\n",
    "\n",
    "# convert sparse matrix to csr matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "corpus_csr = csr_matrix(corpus_sparse)\n",
    "\n",
    "print(\"csr\",corpus_csr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create the TFIDF matrix (corpus) in gensim?\n",
    "https://www.machinelearningplus.com/nlp/gensim-tutorial/#8howtocreatethetfidfmatrixcorpusingensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "# import corpora\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "# Create the Dictionary and Corpus\n",
    "mydict = corpora.Dictionary([nltk_preprocessing(line) for line in data['text']])\n",
    "corpus = [mydict.doc2bow(nltk_preprocessing(line)) for line in data['text']]\n",
    "\n",
    "# create the tfidf model\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSA using Gensim\n",
    "https://machinelearninggeek.com/discovering-hidden-themes-of-documents/#:~:text=Implementing%20LSA%20using%20Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of tfidf values\n",
    "import matplotlib.pyplot as plt\n",
    "tfidf_values = [tfidf[doc] for doc in corpus]\n",
    "tfidf_values = [item for sublist in tfidf_values for item in sublist]\n",
    "tfidf_values = [item[1] for item in tfidf_values]\n",
    "\n",
    "plt.hist(tfidf_values, bins=100)\n",
    "\n",
    "# add axis \n",
    "plt.xlabel('tfidf value')\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "234f4b08dea796048dbaea19d53a00081c31de7324785530dd8e3f9d12a3f19d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
